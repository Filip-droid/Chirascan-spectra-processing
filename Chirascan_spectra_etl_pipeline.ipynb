{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079acb4-3578-47c5-ac42-f64f7b746374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# ABSOLUTE PATH to the directory containing the raw CSV files from Chirascan.\n",
    "# NOTE: We use the 'r' prefix (raw string) to handle backslashes in Windows paths correctly.\n",
    "# Replace the path below with your actual local path.\n",
    "FOLDER_PATH = r'C:\\Users\\Filip\\Documents\\Research\\Biophysics_Project_G4\\Raw_Spectra_Data'\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def parse_chirascan_file(file_path: str) -> Tuple[Optional[str], Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Parses a single Chirascan CSV file to extract metadata (Date) and spectral data.\n",
    "   \n",
    "    Args:\n",
    "        file_path (str): The absolute path to the CSV file.\n",
    "       \n",
    "    Returns:\n",
    "        Tuple[str, pd.DataFrame]: A tuple containing the measurement date and the data.\n",
    "                                  Returns (None, None) if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the file safely to read metadata before loading the bulk data with Pandas.\n",
    "        # 'errors=\"ignore\"' is used to bypass potential encoding issues in older instrument files.\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # 1. METADATA EXTRACTION\n",
    "        # Locate the line containing the date (Format in file: \"#Date: YYYY/MM/DD\")\n",
    "        # We use a generator expression for memory efficiency.\n",
    "        date_line = next((line for line in lines if '#Date:' in line), None)\n",
    "       \n",
    "        # If date is missing, mark as 'Unknown', otherwise strip whitespace.\n",
    "        measurement_date = date_line.split('#Date:')[-1].strip() if date_line else 'Unknown_Date'\n",
    "\n",
    "        # 2. DYNAMIC HEADER DETECTION\n",
    "        # The data section starts after the 'Data:' keyword.\n",
    "        # We need to find the exact line where numerical values begin.\n",
    "        start_line_idx = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'Data:' in line:\n",
    "                # Look ahead to find the first line containing a decimal point (indicating numbers).\n",
    "                # This makes the parser robust against varying header lengths.\n",
    "                start_line_idx = next((j for j, l in enumerate(lines[i+1:], start=i+1) if '.' in l), None)\n",
    "                break\n",
    "       \n",
    "        # Safety check: If no data start is found, log a warning and skip.\n",
    "        if start_line_idx is None:\n",
    "            print(f\"Warning: Could not identify the data start line in file: {os.path.basename(file_path)}\")\n",
    "            return None, None\n",
    "\n",
    "        # Find the end of the data block (first empty line after data starts).\n",
    "        end_line_idx = next((i for i, line in enumerate(lines[start_line_idx:], start=start_line_idx) if not line.strip()), None)\n",
    "       \n",
    "        # Calculate the number of rows to read.\n",
    "        rows_to_read = (end_line_idx - start_line_idx) if end_line_idx else None\n",
    "\n",
    "        # 3. DATA LOADING\n",
    "        # Load only the relevant section into a DataFrame.\n",
    "        # We explicitly select columns 0 (Wavelength) and 1 (Signal/Ellipticity).\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            header=None,\n",
    "            skiprows=start_line_idx,\n",
    "            nrows=rows_to_read,\n",
    "            usecols=[0, 1]\n",
    "        )\n",
    "       \n",
    "        # Handle empty files\n",
    "        if df.empty:\n",
    "            return None, None\n",
    "           \n",
    "        # Standardize column names for downstream processing\n",
    "        df.columns = ['Wavelength', 'Signal']\n",
    "       \n",
    "        return measurement_date, df\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch-all for IO errors or unexpected formats to prevent the pipeline from crashing.\n",
    "        print(f\"Error processing file {os.path.basename(file_path)}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def load_and_group_spectra(folder_path: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Iterates through all CSV files in the target folder, parses them,\n",
    "    and aggregates them by measurement date.\n",
    "   \n",
    "    Returns:\n",
    "        Dict: Keys are dates (str), Values are DataFrames with merged spectra.\n",
    "    \"\"\"\n",
    "    # Use glob to find all CSV files in the directory\n",
    "    search_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "    files = glob.glob(search_pattern)\n",
    "   \n",
    "    if not files:\n",
    "        print(f\"No CSV files found in directory: {folder_path}\")\n",
    "        return {}\n",
    "\n",
    "    # Temporary storage: { '2023/10/20': [ (filename1, df1), (filename2, df2) ] }\n",
    "    data_buffer = {}\n",
    "\n",
    "    print(f\"Initializing batch processing for {len(files)} files...\")\n",
    "\n",
    "    # --- STEP 1: PARSING LOOP ---\n",
    "    for file_path in files:\n",
    "        file_name = os.path.basename(file_path).replace('.csv', '')\n",
    "       \n",
    "        # Parse individual file\n",
    "        date, df = parse_chirascan_file(file_path)\n",
    "\n",
    "        if df is not None:\n",
    "            if date not in data_buffer:\n",
    "                data_buffer[date] = []\n",
    "            data_buffer[date].append((file_name, df))\n",
    "\n",
    "    # --- STEP 2: AGGREGATION ---\n",
    "    grouped_data = {}\n",
    "   \n",
    "    for date, file_list in data_buffer.items():\n",
    "        # We use the Wavelength from the first file as the index (reference X-axis).\n",
    "        # It is assumed all files share the same wavelength range.\n",
    "        base_df = file_list[0][1][['Wavelength']].copy()\n",
    "        base_df.set_index('Wavelength', inplace=True)\n",
    "       \n",
    "        all_signals = []\n",
    "        for name, df in file_list:\n",
    "            # Rename the 'Signal' column to the file name to keep track of samples.\n",
    "            signal_series = df.set_index('Wavelength')['Signal']\n",
    "            signal_series.name = name\n",
    "            all_signals.append(signal_series)\n",
    "           \n",
    "        # Concatenate all series horizontally (axis=1) - Highly efficient operation in Pandas.\n",
    "        merged_df = pd.concat(all_signals, axis=1)\n",
    "       \n",
    "        # Reset index to make Wavelength a regular column again (better for plotting/export).\n",
    "        merged_df.reset_index(inplace=True)\n",
    "        grouped_data[date] = merged_df\n",
    "       \n",
    "    print(f\"Successfully grouped data into {len(grouped_data)} unique dates.\")\n",
    "    return grouped_data\n",
    "\n",
    "def export_to_excel(grouped_data: Dict[str, pd.DataFrame], output_path: str):\n",
    "    \"\"\"\n",
    "    Exports the aggregated data to a multi-section Excel file.\n",
    "    Data blocks are placed side-by-side for easy comparison in Excel.\n",
    "    \"\"\"\n",
    "    if not grouped_data:\n",
    "        print(\"No data to export.\")\n",
    "        return\n",
    "\n",
    "    # Sort dates chronologically to ensure logical order in the report\n",
    "    try:\n",
    "        sorted_dates = sorted(grouped_data.keys(), key=lambda d: datetime.strptime(d, '%Y/%m/%d'))\n",
    "    except ValueError:\n",
    "        # Fallback for non-standard date formats\n",
    "        sorted_dates = sorted(grouped_data.keys())\n",
    "\n",
    "    print(f\"Exporting data to: {output_path}\")\n",
    "\n",
    "    # Use 'openpyxl' engine to write to .xlsx\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        start_col = 0\n",
    "        sheet_name = 'Spectra Analysis'\n",
    "       \n",
    "        # Initialize the sheet\n",
    "        pd.DataFrame().to_excel(writer, sheet_name=sheet_name)\n",
    "        sheet = writer.sheets[sheet_name]\n",
    "\n",
    "        for date in sorted_dates:\n",
    "            df = grouped_data[date]\n",
    "           \n",
    "            # Write the Header (Date) above the data block\n",
    "            sheet.cell(row=1, column=start_col + 1).value = f\"Measurement Date: {date}\"\n",
    "           \n",
    "            # Write the DataFrame\n",
    "            df.to_excel(writer, sheet_name=sheet_name, startrow=1, startcol=start_col, index=False)\n",
    "           \n",
    "            # Shift the starting column for the next block (Width of DF + 1 spacer column)\n",
    "            start_col += len(df.columns) + 1\n",
    "           \n",
    "    print(\"âœ… Export completed successfully.\")\n",
    "\n",
    "# --- MAIN EXECUTION BLOCK ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if the configured path exists\n",
    "    if not os.path.exists(FOLDER_PATH):\n",
    "        print(f\"Error: The path '{FOLDER_PATH}' does not exist.\")\n",
    "        print(\"Please update the 'FOLDER_PATH' variable in the configuration section.\")\n",
    "    else:\n",
    "        # 1. Process Data\n",
    "        results = load_and_group_spectra(FOLDER_PATH)\n",
    "       \n",
    "        # 2. Export Results (Saved in the same folder as input data)\n",
    "        output_file = os.path.join(FOLDER_PATH, 'Processed_Spectra_Report.xlsx')\n",
    "        export_to_excel(results, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
